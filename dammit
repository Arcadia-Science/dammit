#!/usr/bin/env python
from __future__ import print_function

import argparse

from tasks import *

from doit.cmd_base import TaskLoader
from doit.doit_cmd import DoitMain



def run_tasks(tasks, args, config={'verbosity': 2}):
    
    if type(tasks) is not list:
        raise TypeError('tasks must be a list')
   
    class Loader(TaskLoader):
        @staticmethod
        def load_tasks(cmd, opt_values, pos_args):
            return tasks, config
   
    DoitMain(Loader()).run(args)


def prep(resources, config, db_dir, args, cmd='run'):
    '''
    Database prep. These tasks download the necessary databases to the
    database folder and prep them for use. They are gathered under the
    'prep' task group. Current databases being used are:

        * Pfam-A (protein domans)
        * Rfam (RNA models)
        * OrthoDB8 (conserved ortholog groups)
        * uniref90 (protiens, if --full selected)
        * Any user supplied databases

    This set of tasks keeps its own doit db in the db folder to share
    them between all runs.
    '''

    doit_config = {
                    'backend': 'sqlite3',
                    'verbosity': 2
                  }
    db_dir = os.path.abspath(db_dir)
    databases = {}

    # Move into the database directory
    old_dir = os.getcwd()
    try:
        try:
            os.mkdir(db_dir)
        except OSError:
            pass
        os.chdir(db_dir)

        prep_tasks = []

        # Get Pfam-A and prepare it for use with hmmer
        PFAM = resources['pfam']['filename']
        prep_tasks.append(
            get_download_and_gunzip_task(resources['pfam']['url'], PFAM)
        )
        prep_tasks.append(
            get_hmmpress_task(PFAM)
        )
        databases['PFAM'] = os.path.abspath(PFAM)

        # Get Rfam and prepare it for use with Infernal
        RFAM = resources['rfam']['filename']
        prep_tasks.append(
            get_download_and_gunzip_task(resources['rfam']['url'], RFAM)
        )
        prep_tasks.append(
            get_cmpress_task(RFAM)
        )
        databases['RFAM'] = os.path.abspath(RFAM)

        # Get OrthoDB and prepare it for BLAST use
        ORTHODB = resources['orthodb']['filename']
        prep_tasks.append(
            get_download_and_gunzip_task(resources['orthodb']['url'], ORTHODB)
        )
        prep_tasks.append(
            get_blast_format_task(ORTHODB, ORTHODB + '.db', 
                                  resources['orthodb']['db_type'])
        )
        ORTHODB += '.db'
        databases['ORTHODB'] = os.path.abspath(ORTHODB)

        busco_db = args.busco_group
        prep_tasks.append(
            get_download_and_untar_task(resources['busco'][busco_db]['url'], 
                                        'buscodb',
                                        label=busco_db)
        )
        databases['BUSCO'] = os.path.abspath(os.path.join('buscodb', busco_db))

        # Get uniref90 if the user specifies
        if args.full:
            UNIREF = resources['uniref90']['filename']
            prep_tasks.append(
                get_download_and_gunzip_task(resources['uniref90']['url'], UNIREF)
            )
            prep_tasks.append(
                get_blast_format_task(UNIREF, UNIREF + '.db',
                                      resources['uniref90']['db_type'])
            )
            UNIREF += '.db'
            databases['UNIREF'] = os.path.abspath(UNIREF)

        if args.debug:
            print_tasks(prep_tasks)
            cmd = 'list'
        run_tasks(prep_tasks, [cmd], config=doit_config)

    finally:
        os.chdir(old_dir)

    return databases

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--transcriptome', required=True)
    parser.add_argument('--output-dir', default=None)
    parser.add_argument('--database-dir', default=None)
    parser.add_argument('--busco-group', default='metazoa')
    parser.add_argument('--n_threads', type=int, default=1)
    parser.add_argument('--user-databases', nargs='+')
    parser.add_argument('--debug', action='store_true', default=False)
    parser.add_argument('--full', action='store_true', default=False,
                        help='Do full annotation with uniref90')
    args, doit_args = parser.parse_known_args()
    # Get the config metadata
    with open('resources.json', 'r') as fp:
        resources = json.load(fp)
    with open('config.json', 'r') as fp:
        config = json.load(fp)

    # Print out the infos~
    print(config['meta']['description'], file=sys.stderr)
    print(', '.join(config['meta']['authors']), config['meta']['date'], file=sys.stderr)

    # By default, we store databases in the home directory
    db_dir = args.database_dir
    if db_dir is None:
        db_dir = os.path.join(os.environ['HOME'], config['settings']['db_dir'])
    db_dir = os.path.abspath(db_dir)
    print('DB_DIR', db_dir, file=sys.stderr)

    transcriptome = args.transcriptome
    out_dir = args.output_dir
    if out_dir is None:
        out_dir = transcriptome + '.dammit'
    out_dir = os.path.abspath(out_dir)

    try:
        os.mkdir(out_dir)
    except OSError:
        # Already exists, carry on
        pass

    # Main task list
    TASKS = []

    '''
    Set up all the databases. First, run prep(...) to get all the
    dammit databases. Then, prep the user supplied databases in
    the output directory.
    '''

    DAMMIT_DATABASES = prep(resources, config, db_dir, args, cmd='run')

    USER_DATABASES = []
    if args.user_databases:
        for db in args.databases:
            db_name = os.path.join(out_dir, db + '.db')
            TASKS.append(
                get_blast_format_task(db, db_name, 'prot')
            )
            USER_DATABASES.append(db_name)

    '''
    Set up doit's config for the actual analysis tasks.
    We'll put the doit database for these tasks into the output
    directory so that we don't end up scattering them around the
    filesystem, or worse, with one master db containing dependency
    metadata from every analysis ever run by the user!
    '''

    doit_config = {
                    'backend': 'sqlite3',
                    'verbosity': 2,
                    'dep_file': os.path.join(out_dir, '.' + os.path.basename(transcriptome) + '.doit.db')
                  }

    '''
    Calculate assembly information. First it runs some basic stats like N50 and
    number of contigs, and uses the HyperLogLog counter from khmer to
    estimate unique k-mers for checking redundancy. Then it runs BUSCO to
    assess completeness. These tasks are grouped under the 'assess' task.
    '''

    assess_tasks = []
    assess_tasks.append(
        get_transcriptome_stats_task(transcriptome, out_dir)
    )
    
    TASKS.extend(assess_tasks)
    TASKS.append(get_group_task('assess', assess_tasks))

    '''
    BLAST
    '''
    '''
    blast_iters = []
    for fn in resources_df[resources_df.meta_type == 'assembly'].filename:

        blast_iters.extend([blast_task(row, config, fn) \
                    for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                               (resources_df.meta_type != 'assembly')].iterrows()])

    blast_iters.extend([blast_task(row, config, resources_df.ix['petMar2_cdna'].filename) \
                    for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                               (resources_df.filename != 'petMar2.cdna.fa')].iterrows()])

    blast_tasks = []
    for tskiter in blast_iters:
        blast_tasks.extend([dict_to_task(tsk) for tsk in tskiter])
    tasks.extend(blast_tasks)
    tasks.append(group_task('blast', [t.name for t in blast_tasks]))
    '''
    '''
    TransDecoder and hmmscan
    '''
    '''
    tdc_tasks = []
    dbfn = resources_df.ix['pfamA'].filename
    tdc_tasks.append(transdecoder_orf_task(assembly_fn, config['pipeline']['transdecoder']))
    pep_fn = os.path.join(assembly_fn+'.transdecoder_dir', 'longest_orfs.pep')
    tdc_tasks.append(hmmscan_task(pep_fn, assembly_fn + '.pfam-A.out', dbfn, 
                                  config['pipeline']['hmmscan']))
    tdc_tasks.append(transdecoder_predict_task(assembly_fn, assembly_fn + '.pfam-A.out',
                     config['pipeline']['transdecoder']))
    tasks.extend(tdc_tasks)
    tasks.append(group_task('transdecoder', [t.name for t in tdc_tasks]))
    '''
    '''
    BUSCO
    '''
    '''
    busco_cfg = config['pipeline']['busco']
    busco_tasks = []

    busco_tasks.append(busco_vert_db_task)
    busco_tasks.append(busco_metz_db_task)

    for fn in [assembly_fn, resources_df.ix['petMar2_cdna'].filename]:
        for db in ['metazoa', 'vertebrata']:
            output_dir = '.'.join([fn, db, busco_cfg['output_suffix']])
            busco_tasks.append(busco_task(fn, output_dir, 
                               os.path.join(busco_cfg['db_dir'], db), 
                               'trans', busco_cfg))

    tasks.extend(busco_tasks)
    tasks.append(group_task('busco', [t.name for t in busco_tasks]))
    '''
    '''
    infernal and Rfam
    '''
    '''
    cmscan_cfg = config['pipeline']['cmscan']
    cmscan_tasks = []
    cmscan_tasks.append(download_and_untar_task(resources_df.ix['rfam'].url,
                                                cmscan_cfg['db_dir'],
                                                label='rfam'))


    ann_task = aggregate_annotations_task(assembly_fn, blast_targets, 
                                          assembly_fn+'.transdecoder.gff3',
                                          assembly_fn+'.pfam-A.out', sample_df,
                                          assembly_fn+'.eXpress.tpm.tsv',
                                          assembly_fn+'.annotations.h5')
    tasks.append(ann_task)
    '''
    
    if args.debug:
        print_tasks(TASKS)

    if doit_args:
        run_tasks(TASKS, doit_args, config=doit_config)


main()
