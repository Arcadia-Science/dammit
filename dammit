#!/usr/bin/env python
from __future__ import print_function

import argparse

from tasks import *

from doit.cmd_base import TaskLoader
from doit.doit_cmd import DoitMain

def run_tasks(tasks, args, config={'verbosity': 2}):
    
    if type(tasks) is not list:
        raise TypeError('tasks must be a list')
   
    class Loader(TaskLoader):
        @staticmethod
        def load_tasks(cmd, opt_values, pos_args):
            return tasks, config
   
    DoitMain(Loader()).run(args)

def get_database_prep_tasks(resources_df):
    '''
    Database prep, homology search
    
    First, we get remote flat files.
    '''
    tasks = []
    for key, row in resources_df[resources_df.access == 'remote_file'].iterrows():
        tasks.append(download_and_gunzip_task(row.url, row.filename))
    
    # Programmatically query uniprot
    for key, row in  resources_df[(resources_df.access == 'remote_query') & \
                                  (resources_df.q_type == 'uniprot')].iterrows():
   
        tasks.append(uniprot_query_task(row.terms, row.filename, label=row.filename))

    # Prep HMM profiles
    for key, row in resources_df[resources_df.meta_type == 'hmm_profiles'].iterrows():
        tasks.append(hmmpress_task(row.filename))

    # Truncate the long fasta names so blast doesn't choke
    for key, row in resources_df[resources_df.meta_type == 'fasta_database'].iterrows():
        tasks.append(truncate_fasta_header_task(row.filename))

    # Generate blast indices
    for key, row in resources_df[resources_df.meta_type.isin(['fasta_database', 'assembly'])].iterrows():
        tasks.append(blast_format_task(row.filename, row.filename + '.db', row.db_type))

    return tasks

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--transcriptome', required=True)
    parser.add_argument('--output-dir', default=None)
    parser.add_argument('--n_threads', type=int, default=1)
    parser.add_argument('--databases', nargs='+')
    parser.add_argument('--full', action='store_true', default=False,
                        help='Do full annotation with uniref90')
    args, doit_args = parser.parse_known_args()

    # Get the config metadata
    with open('resources.json', 'r') as fp:
        resources = json.load(fp)
    with open('config.json', 'r') as fp:
        config = json.load(fp)

    # Print out the infos~
    print(config['meta']['description'], file=sys.stderr)
    print(', '.join(config['meta']['authors']), config['meta']['date'], file=sys.stderr)

    # Functions to join the database and work dirs with a file
    def work_dir(fn):
        return os.path.join(config['settings']['work_dir'], fn)
    def db_dir(fn):
        return os.path.join(config['settings']['db_dir'], fn)

    transcriptome = args.transcriptome
    out_dir = args.output_dir
    if out_dir is None:
        out_dir = transcriptome + '.dammit'

    # Master list of tasks
    tasks = []

    '''
    Database prep. These tasks download the necessary databases to the
    database folder and prep them for use. They are gathered under the
    'prep' task group. Current databases being used are:
        * Pfam-A (protein domans)
        * Rfam (RNA models)
        * OrthoDB8 (conserved ortholog groups)
        * uniref90 (protiens, if --full selected)
        * Any user supplied databases
    '''

    prep_tasks = []

    # Create directories
    prep_tasks.append(get_create_folder_task(work_dir))
    prep_tasks.append(get_create_folder_task(db_dir))
    prep_tasks.append(get_create_folder_task(out_dir))

    # Get Pfam-A and prepare it for use with hmmer
    PFAM = db_dir(resources['pfam']['filename'])
    prep_tasks.append(
        get_download_and_gunzip_task(resources['pfam']['url'], PFAM)
    )
    prep_tasks.append(
        get_hmmpress_task(PFAM)
    )

    # Get Rfam and prepare it for use with Infernal
    RFAM = db_dir(resources['rfam']['filename'])
    prep_tasks.append(
        get_download_and_gunzip_task(resources['rfam']['url'], RFAM)
    )
    prep_tasks.append(
        get_cmpress_task(RFAM)
    )

    # Get OrthoDB and prepare it for BLAST use
    ORTHODB = db_dir(resources['orthodb']['filename'])
    prep_tasks.append(
        get_download_and_gunzip_task(resources['orthodb']['url'], ORTHODB)
    )
    prep_tasks.append(
        get_blast_format_task(ORTHODB, ORTHODB + '.db', 
                              resources['orthodb']['db_type'])
    )
    ORTHODB += '.db'

    # Get uniref90 if the user specifies
    if args.full:
        UNIREF = db_dir(resources['uniref90']['filename'])
        prep_tasks.append(
            get_download_and_gunzip_task(resources['uniref90']['url'], UNIREF)
        )
        prep_tasks.append(
            get_blast_format_task(UNIREF, UNIREF + '.db',
                                  resources['uniref90']['db_type'])
        )
        UNIREF += '.db'

    # If there are user-supplied databases, use them
    USER_DATABASES = []
    if args.databases:
        for db in args.databases:
            db_name = db_dir(db + '.db')
            prep_tasks.append(
                get_blast_format_task(db, db_name, 'prot')
            )
            USER_DATABASES.append(db_name)

    # Make a task group specifically for the database prep tasks
    tasks.extend(prep_tasks)
    tasks.append(get_group_task('prep', prep_tasks))


    '''
    Calculate assembly information. First it runs some basic stats like N50 and
    number of contigs, and uses the HyperLogLog counter from khmer to
    estimate unique k-mers for checking redundancy. Then it runs BUSCO to
    assess completeness. These tasks are grouped under the 'assess' task.
    '''

    assess_tasks = []
    assess_tasks.append(
        get_transcriptome_stats_task(transcriptome, out_dir)
    )
    
    tasks.extend(assess_tasks)
    tasks.append(get_group_task('assess', assess_tasks))

    '''
    BLAST
    '''
    '''
    blast_iters = []
    for fn in resources_df[resources_df.meta_type == 'assembly'].filename:

        blast_iters.extend([blast_task(row, config, fn) \
                    for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                               (resources_df.meta_type != 'assembly')].iterrows()])

    blast_iters.extend([blast_task(row, config, resources_df.ix['petMar2_cdna'].filename) \
                    for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                               (resources_df.filename != 'petMar2.cdna.fa')].iterrows()])

    blast_tasks = []
    for tskiter in blast_iters:
        blast_tasks.extend([dict_to_task(tsk) for tsk in tskiter])
    tasks.extend(blast_tasks)
    tasks.append(group_task('blast', [t.name for t in blast_tasks]))
    '''
    '''
    TransDecoder and hmmscan
    '''
    '''
    tdc_tasks = []
    dbfn = resources_df.ix['pfamA'].filename
    tdc_tasks.append(transdecoder_orf_task(assembly_fn, config['pipeline']['transdecoder']))
    pep_fn = os.path.join(assembly_fn+'.transdecoder_dir', 'longest_orfs.pep')
    tdc_tasks.append(hmmscan_task(pep_fn, assembly_fn + '.pfam-A.out', dbfn, 
                                  config['pipeline']['hmmscan']))
    tdc_tasks.append(transdecoder_predict_task(assembly_fn, assembly_fn + '.pfam-A.out',
                     config['pipeline']['transdecoder']))
    tasks.extend(tdc_tasks)
    tasks.append(group_task('transdecoder', [t.name for t in tdc_tasks]))
    '''
    '''
    BUSCO
    '''
    '''
    busco_cfg = config['pipeline']['busco']
    busco_tasks = []
    busco_vert_db_task = download_and_untar_task(busco_cfg['vert_url'], 
                                                 busco_cfg['db_dir'],
                                                 label='vertebrata')
    busco_metz_db_task = download_and_untar_task(busco_cfg['metazoa_url'],
                                                 busco_cfg['db_dir'],
                                                 label='metazoa')
    busco_tasks.append(busco_vert_db_task)
    busco_tasks.append(busco_metz_db_task)

    for fn in [assembly_fn, resources_df.ix['petMar2_cdna'].filename]:
        for db in ['metazoa', 'vertebrata']:
            output_dir = '.'.join([fn, db, busco_cfg['output_suffix']])
            busco_tasks.append(busco_task(fn, output_dir, 
                               os.path.join(busco_cfg['db_dir'], db), 
                               'trans', busco_cfg))

    tasks.extend(busco_tasks)
    tasks.append(group_task('busco', [t.name for t in busco_tasks]))
    '''
    '''
    infernal and Rfam
    '''
    '''
    cmscan_cfg = config['pipeline']['cmscan']
    cmscan_tasks = []
    cmscan_tasks.append(download_and_untar_task(resources_df.ix['rfam'].url,
                                                cmscan_cfg['db_dir'],
                                                label='rfam'))


    ann_task = aggregate_annotations_task(assembly_fn, blast_targets, 
                                          assembly_fn+'.transdecoder.gff3',
                                          assembly_fn+'.pfam-A.out', sample_df,
                                          assembly_fn+'.eXpress.tpm.tsv',
                                          assembly_fn+'.annotations.h5')
    tasks.append(ann_task)
    '''
    '''
    if args.print_tasks:
        for task in tasks:
            print('-----\n', task)
            pprint.pprint(task.__dict__)
    '''
    if doit_args:
        run_tasks(tasks, doit_args)


main()
