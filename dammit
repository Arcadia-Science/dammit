#!/usr/bin/env python
from __future__ import print_function

import argparse

from tasks import *

from doit.cmd_base import TaskLoader
from doit.doit_cmd import DoitMain



def run_tasks(tasks, args, config={'verbosity': 2}):
    
    if type(tasks) is not list:
        raise TypeError('tasks must be a list')
   
    class Loader(TaskLoader):
        @staticmethod
        def load_tasks(cmd, opt_values, pos_args):
            return tasks, config
   
    DoitMain(Loader()).run(args)


def install(resources, config, db_dir, args, cmd='run'):
    '''


    Current binary distributions being used are:


    This set of tasks keeps its own doit db in the db folder to share
    them between all runs.
    '''
    

    doit_config = {
                    'backend': 'sqlite3',
                    'verbosity': 2,
                    'dep_file': dep_file
                  }

    db_dir = os.path.abspath(db_dir)
    databases = {}



def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--transcriptome', required=True)
    parser.add_argument('--output-dir', default=None)
    parser.add_argument('--database-dir', default=None)
    parser.add_argument('--busco-group', default='metazoa')
    parser.add_argument('--n_threads', type=int, default=1)
    parser.add_argument('--user-databases', nargs='+')
    parser.add_argument('--debug', action='store_true', default=False)
    parser.add_argument('--full', action='store_true', default=False,
                        help='Do full annotation with uniref90')
    args, doit_args = parser.parse_known_args()
    # Get the config metadata
    with open('resources.json', 'r') as fp:
        resources = json.load(fp)
    with open('config.json', 'r') as fp:
        config = json.load(fp)

    # Print out the infos~
    print(config['meta']['description'], file=sys.stderr)
    print(', '.join(config['meta']['authors']), config['meta']['date'], 
          file=sys.stderr)

    # By default, we store databases in the home directory
    db_dir = args.database_dir
    if db_dir is None:
        db_dir = os.path.join(os.environ['HOME'], 
                              config['settings']['db_dir'])
    db_dir = os.path.abspath(db_dir)
    print('DB_DIR', db_dir, file=sys.stderr)

    transcriptome = args.transcriptome
    out_dir = args.output_dir
    if out_dir is None:
        out_dir = transcriptome + '.dammit'
    out_dir = os.path.abspath(out_dir)

    try:
        os.mkdir(out_dir)
    except OSError:
        # Already exists, carry on
        pass

    # Main task list
    TASKS = []

    '''
    Set up all the databases. First, run prep(...) to get all the
    dammit databases. Then, prep the user supplied databases in
    the output directory.
    '''

    DAMMIT_DATABASES = prep(resources, config, db_dir, args, cmd='run')

    USER_DATABASES = []
    if args.user_databases:
        for db in args.databases:
            db_name = os.path.join(out_dir, db + '.db')
            TASKS.append(
                get_blast_format_task(db, db_name, 'prot')
            )
            USER_DATABASES.append(db_name)

    '''
    Set up doit's config for the actual analysis tasks.
    We'll put the doit database for these tasks into the output
    directory so that we don't end up scattering them around the
    filesystem, or worse, with one master db containing dependency
    metadata from every analysis ever run by the user!
    '''

    doit_config = {
                    'backend': 'sqlite3',
                    'verbosity': 2,
                    'dep_file': os.path.join(out_dir, '.' +
                                             os.path.basename(transcriptome) +
                                             '.doit.db')
                  }

    '''
    Calculate assembly information. First it runs some basic stats like N50 and
    number of contigs, and uses the HyperLogLog counter from khmer to
    estimate unique k-mers for checking redundancy. Then it runs BUSCO to
    assess completeness. These tasks are grouped under the 'assess' task.
    '''

    assess_tasks = []
    assess_tasks.append(
        get_transcriptome_stats_task(transcriptome, out_dir)
    )
    
    TASKS.extend(assess_tasks)
    TASKS.append(get_group_task('assess', assess_tasks))

    '''
    BLAST
    '''
    '''
    blast_iters = []
    for fn in resources_df[resources_df.meta_type == 'assembly'].filename:

        blast_iters.extend([blast_task(row, config, fn) \
                    for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                               (resources_df.meta_type != 'assembly')].iterrows()])

    blast_iters.extend([blast_task(row, config, resources_df.ix['petMar2_cdna'].filename) \
                    for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                               (resources_df.filename != 'petMar2.cdna.fa')].iterrows()])

    blast_tasks = []
    for tskiter in blast_iters:
        blast_tasks.extend([dict_to_task(tsk) for tsk in tskiter])
    tasks.extend(blast_tasks)
    tasks.append(group_task('blast', [t.name for t in blast_tasks]))
    '''
    '''
    TransDecoder and hmmscan
    '''
    '''
    tdc_tasks = []
    dbfn = resources_df.ix['pfamA'].filename
    tdc_tasks.append(transdecoder_orf_task(assembly_fn, config['pipeline']['transdecoder']))
    pep_fn = os.path.join(assembly_fn+'.transdecoder_dir', 'longest_orfs.pep')
    tdc_tasks.append(hmmscan_task(pep_fn, assembly_fn + '.pfam-A.out', dbfn, 
                                  config['pipeline']['hmmscan']))
    tdc_tasks.append(transdecoder_predict_task(assembly_fn, assembly_fn + '.pfam-A.out',
                     config['pipeline']['transdecoder']))
    tasks.extend(tdc_tasks)
    tasks.append(group_task('transdecoder', [t.name for t in tdc_tasks]))
    '''
    '''
    BUSCO
    '''
    busco_cfg = config['settings']['busco']
    for fn in [assembly_fn, resources_df.ix['petMar2_cdna'].filename]:
        for db in ['metazoa', 'vertebrata']:
            output_dir = '.'.join([fn, db, busco_cfg['output_suffix']])
            busco_tasks.append(busco_task(fn, output_dir, 
                               os.path.join(busco_cfg['db_dir'], db), 
                               'trans', busco_cfg))

    tasks.extend(busco_tasks)
    tasks.append(group_task('busco', [t.name for t in busco_tasks]))
    '''
    infernal and Rfam
    '''
    '''
    cmscan_cfg = config['pipeline']['cmscan']
    cmscan_tasks = []
    cmscan_tasks.append(download_and_untar_task(resources_df.ix['rfam'].url,
                                                cmscan_cfg['db_dir'],
                                                label='rfam'))


    ann_task = aggregate_annotations_task(assembly_fn, blast_targets, 
                                          assembly_fn+'.transdecoder.gff3',
                                          assembly_fn+'.pfam-A.out', sample_df,
                                          assembly_fn+'.eXpress.tpm.tsv',
                                          assembly_fn+'.annotations.h5')
    tasks.append(ann_task)
    '''
    
    if args.debug:
        print_tasks(TASKS)

    if doit_args:
        run_tasks(TASKS, doit_args, config=doit_config)


main()
